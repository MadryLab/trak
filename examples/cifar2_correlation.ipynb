{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f060ac-ecb3-4129-9ea9-238b3fd7afaa",
   "metadata": {},
   "source": [
    "# Train and save model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec6fa04-de0a-4e73-8b35-a4f08bf871c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ffcv.fields.decoders import IntDecoder, SimpleRGBImageDecoder\n",
    "from ffcv.loader import Loader, OrderOption\n",
    "from ffcv.pipeline.operation import Operation\n",
    "from ffcv.transforms import RandomHorizontalFlip, Cutout, \\\n",
    "    RandomTranslate, Convert, ToDevice, ToTensor, ToTorchImage\n",
    "from ffcv.transforms.common import Squeeze\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import wget\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn import CrossEntropyLoss, Conv2d, BatchNorm2d\n",
    "from torch.optim import SGD, lr_scheduler\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f949d611-c408-4fe0-a00a-e68006e766fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BETONS = {\n",
    "        'train': \"https://www.dropbox.com/s/zn7jsp2rl09e0fh/train.beton?dl=1\",\n",
    "        'val': \"https://www.dropbox.com/s/4p73milxxafv4cm/val.beton?dl=1\",\n",
    "}\n",
    "\n",
    "STATS = {\n",
    "        'mean': [125.307, 122.961, 113.8575],\n",
    "        'std': [51.5865, 50.847, 51.255]\n",
    "}\n",
    "\n",
    "def get_dataloader(batch_size=256,\n",
    "                num_workers=8,\n",
    "                split='train',  # split \\in [train, val]\n",
    "                aug_seed=0,\n",
    "                order='sequential',\n",
    "                subsample=False,\n",
    "                should_augment=True,\n",
    "                indices=None):\n",
    "        label_pipeline: List[Operation] = [IntDecoder(),\n",
    "                                        ToTensor(),\n",
    "                                        ToDevice(torch.device('cuda:0')),\n",
    "                                        Squeeze()]\n",
    "        image_pipeline: List[Operation] = [SimpleRGBImageDecoder()]\n",
    "\n",
    "        if should_augment:\n",
    "                image_pipeline.extend([\n",
    "                        RandomHorizontalFlip(),\n",
    "                        RandomTranslate(padding=2, fill=tuple(map(int, STATS['mean']))),\n",
    "                        Cutout(4, tuple(map(int, STATS['mean']))),\n",
    "                ])\n",
    "\n",
    "        image_pipeline.extend([\n",
    "            ToTensor(),\n",
    "            ToDevice(torch.device('cuda:0'), non_blocking=True),\n",
    "            ToTorchImage(),\n",
    "            Convert(torch.float32),\n",
    "            torchvision.transforms.Normalize(STATS['mean'], STATS['std']),\n",
    "        ])\n",
    "\n",
    "        beton_url = BETONS[split]\n",
    "        beton_path = f'./{split}.beton'\n",
    "        wget.download(beton_url, out=str(beton_path), bar=None)\n",
    "        \n",
    "        if subsample and split == 'train':\n",
    "            indices = np.random.choice(np.arange(10_000), replace=False, size=5_000)\n",
    "\n",
    "        if order == 'sequential':\n",
    "            order = OrderOption.SEQUENTIAL\n",
    "        else:\n",
    "            order = OrderOption.RANDOM\n",
    "        \n",
    "        return Loader(beton_path,\n",
    "                    batch_size=batch_size,\n",
    "                    num_workers=num_workers,\n",
    "                    order=order,\n",
    "                    drop_last=False,\n",
    "                    seed=aug_seed,\n",
    "                    indices=indices,\n",
    "                    pipelines={'image': image_pipeline, 'label': label_pipeline})\n",
    "\n",
    "\n",
    "# Resnet9\n",
    "class Mul(torch.nn.Module):\n",
    "    def __init__(self, weight):\n",
    "        super(Mul, self).__init__()\n",
    "        self.weight = weight\n",
    "    def forward(self, x): return x * self.weight\n",
    "\n",
    "\n",
    "class Flatten(torch.nn.Module):\n",
    "    def forward(self, x): return x.view(x.size(0), -1)\n",
    "\n",
    "\n",
    "class Residual(torch.nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super(Residual, self).__init__()\n",
    "        self.module = module\n",
    "    def forward(self, x): return x + self.module(x)\n",
    "\n",
    "\n",
    "def construct_rn9(num_classes=2):\n",
    "    def conv_bn(channels_in, channels_out, kernel_size=3, stride=1, padding=1, groups=1):\n",
    "        return torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(channels_in, channels_out, kernel_size=kernel_size,\n",
    "                            stride=stride, padding=padding, groups=groups, bias=False),\n",
    "                torch.nn.BatchNorm2d(channels_out),\n",
    "                torch.nn.ReLU(inplace=True)\n",
    "        )\n",
    "    model = torch.nn.Sequential(\n",
    "        conv_bn(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "        conv_bn(64, 128, kernel_size=5, stride=2, padding=2),\n",
    "        Residual(torch.nn.Sequential(conv_bn(128, 128), conv_bn(128, 128))),\n",
    "        conv_bn(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "        torch.nn.MaxPool2d(2),\n",
    "        Residual(torch.nn.Sequential(conv_bn(256, 256), conv_bn(256, 256))),\n",
    "        conv_bn(256, 128, kernel_size=3, stride=1, padding=0),\n",
    "        torch.nn.AdaptiveMaxPool2d((1, 1)),\n",
    "        Flatten(),\n",
    "        torch.nn.Linear(128, num_classes, bias=False),\n",
    "        Mul(0.2)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train(model, loader, lr=0.4, epochs=100, momentum=0.9, weight_decay=5e-4, lr_peak_epoch=5, label_smoothing=0.0):\n",
    "    opt = SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    iters_per_epoch = len(loader)\n",
    "    # Cyclic LR with single triangle\n",
    "    lr_schedule = np.interp(np.arange((epochs+1) * iters_per_epoch),\n",
    "                            [0, lr_peak_epoch * iters_per_epoch, epochs * iters_per_epoch],\n",
    "                            [0, 1, 0])\n",
    "    scheduler = lr_scheduler.LambdaLR(opt, lr_schedule.__getitem__)\n",
    "    scaler = GradScaler()\n",
    "    loss_fn = CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        for it, (ims, labs) in enumerate(loader):\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with autocast():\n",
    "                out = model(ims)\n",
    "                loss = loss_fn(out, labs)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "def evaluate(model, loader_val):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_correct, total_num = 0., 0.\n",
    "        for ims, labs in tqdm(loader):\n",
    "            with autocast():\n",
    "                out = model(ims)\n",
    "                total_correct += out.argmax(1).eq(labs).sum().cpu().item()\n",
    "                total_num += ims.shape[0]\n",
    "        print(f'Test accuracy: {total_correct / total_num * 100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb400b1-9646-4352-a803-ae8cb1ee7b1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs('./checkpoints_cifar2', exist_ok=True)\n",
    "\n",
    "for i in tqdm(range(5), desc='Training models..'):\n",
    "    model = construct_rn9().to(memory_format=torch.channels_last).cuda()\n",
    "    loader_train = get_dataloader(batch_size=512, split='train', order='random', subsample=True)\n",
    "    train(model, loader_train)\n",
    "    \n",
    "    torch.save(model.state_dict(), f'./checkpoints_cifar2/sd_{i}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73561d60-eb99-462a-9c1b-93fa6db800b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ckpt_files = list(Path('./models_cifar2').rglob('model_sd_99.pt'))\n",
    "ckpts = [torch.load(ckpt, map_location='cpu') for ckpt in ckpt_files]\n",
    "\n",
    "ckpt_files_old = list(Path('/mnt/cfs/projects/better_tracin/checkpoints/resnet9_cifar2/50pct/debug').rglob('*.pt'))\n",
    "ckpts_old = [torch.load(ckpt, map_location='cpu') for ckpt in ckpt_files_old]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34185f06-482c-47a8-b51c-020fd13df7b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = get_dataloader(split='val')\n",
    "model = construct_rn9().to(memory_format=torch.channels_last).cuda()\n",
    "model.load_state_dict(ckpts[1])\n",
    "\n",
    "evaluate(model, loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ff92a8-1d9e-4e63-8a2b-63f97085d020",
   "metadata": {},
   "source": [
    "# Set up the TRAKer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cf723c-d1b2-425b-a1e6-265bdedbae77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = construct_rn9().to(memory_format=torch.channels_last).cuda()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb047af-0c14-425d-b1ea-5b2ba7e6b7e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trak.projectors import BasicProjector\n",
    "from trak import TRAKer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9804779-3229-47a2-84d8-771eb7568e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "traker = TRAKer(model=model,\n",
    "                task='image_classification',\n",
    "                proj_dim=2048,\n",
    "                save_dir='./trak_results_cifar_2_debug_2k',\n",
    "                train_set_size=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188a1582-b725-4fa7-94da-51b5bd62b216",
   "metadata": {},
   "source": [
    "# Compute TRAK features for train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89886b24-27f2-4876-9e56-a7ed368fe3c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "loader_train = get_dataloader(batch_size=batch_size, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bbce69-67d0-49d6-8467-6f8602c89154",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model_id, ckpt in enumerate(tqdm(ckpts)):\n",
    "    traker.load_checkpoint(ckpt, model_id=model_id)\n",
    "    for batch in loader_train:\n",
    "        traker.featurize(batch=batch, num_samples=batch[0].shape[0])\n",
    "\n",
    "traker.finalize_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18430d04-e09b-4ec3-8104-8b86bc468c9b",
   "metadata": {},
   "source": [
    "# Compute TRAK scores for targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d6fbe7-2d76-47b5-8f35-a42d63705480",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader_targets = get_dataloader(batch_size=batch_size, split='val', should_augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6629868-7026-4e9f-bbea-1d69515d6944",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model_id, ckpt in enumerate(tqdm(ckpts)):\n",
    "    traker.start_scoring_checkpoint(ckpt,\n",
    "                                    model_id=model_id,\n",
    "                                    num_targets=len(loader_targets.indices))\n",
    "    for batch in loader_targets:\n",
    "        traker.score(batch=batch, num_samples=batch[0].shape[0])\n",
    "\n",
    "scores = traker.finalize_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d76d04-8b6d-4c46-b616-d1e5f4ca488e",
   "metadata": {},
   "source": [
    "# Visualize the attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d477cf-f12d-4a04-b480-f16c72e7d121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8417c903-f291-4185-b0e5-ba32dd45b346",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "targets = [85, 100]  # let's look at two validation images\n",
    "loader_targets = get_dataloader(batch_size=2, split='val', indices=targets, should_augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2678eba-4d19-406c-ad2a-440d85c9b55b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for batch in loader_targets:\n",
    "    ims, _ = batch\n",
    "    ims = (ims - ims.min()) / (ims.max() - ims.min())\n",
    "    for image in ims:\n",
    "        plt.figure(figsize=(1.5,1.5))\n",
    "        plt.imshow(image.cpu().permute([1, 2, 0]).numpy()); plt.axis('off'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b337144-06f0-460a-8440-3d153811807f",
   "metadata": {},
   "source": [
    "And the highest scoring examples in the train set according to TRAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed6b44c-9d44-4081-8db7-fb5fff4c4779",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores_prev = np.load('/mnt/cfs/projects/better_tracin/estimators/CIFAR2/ablation_jl_dim/100models_1epochs_jl1000/estimates.npy')\n",
    "S = scores_prev\n",
    "# S = scores.cpu()\n",
    "\n",
    "for target in targets:\n",
    "    print(f'Top scorers for target {target}')\n",
    "    loader_top_scorer = get_dataloader(batch_size=3, split='train', indices=S[:, target].argsort()[-3:],\n",
    "                                       should_augment=False)\n",
    "    for batch in loader_top_scorer:\n",
    "        ims, _ = batch\n",
    "        ims = (ims - ims.min()) / (ims.max() - ims.min())\n",
    "        for image in ims:\n",
    "            plt.figure(figsize=(1.5, 1.5))\n",
    "            plt.imshow(image.cpu().permute([1, 2, 0]).numpy()); plt.axis('off'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23a8f6a-72c7-445c-ae09-280396d8e518",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scores_prev = np.load('/mnt/cfs/projects/better_tracin/estimators/CIFAR2/ablation_jl_dim/100models_1epochs_jl1000/estimates.npy')\n",
    "# S = scores_prev\n",
    "S = scores.cpu()\n",
    "\n",
    "for target in targets:\n",
    "    print(f'Top scorers for target {target}')\n",
    "    loader_top_scorer = get_dataloader(batch_size=3, split='train', indices=S[:, target].argsort()[-3:],\n",
    "                                       should_augment=False)\n",
    "    for batch in loader_top_scorer:\n",
    "        ims, _ = batch\n",
    "        ims = (ims - ims.min()) / (ims.max() - ims.min())\n",
    "        for image in ims:\n",
    "            plt.figure(figsize=(1.5, 1.5))\n",
    "            plt.imshow(image.cpu().permute([1, 2, 0]).numpy()); plt.axis('off'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbe341a-681c-4ea0-9685-18910e32485c",
   "metadata": {},
   "source": [
    "# Extra: evaluate counterfactuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d1e013-942a-426f-a6c0-327857c284bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "We exactly follow the steps in https://docs.ffcv.io/ffcv_examples/cifar10.html, except for the fact that we replace the CIFAR-10 dataloader with the CIFAR-2 dataloader above. Additionally, we train on *subsets* of CIFAR-2, parametrized by the `masks` arrays below. We collect the model outputs for each retraining on a different subset (mask) in a separate array `margins`.\n",
    "\n",
    "We train a total of 10,000 models. Note that this is not necessary to get TRAK scores. This step is only necessary to get (very high quality) LDS correlation estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e26aa26-63ee-48f5-8c6f-d36727f7ca98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EVAL_DIR = Path('/mnt/cfs/home/spark/store/kernel/cifar2/50pct_new_augs_10x_per_mask')\n",
    "indices = np.where(np.load(EVAL_DIR / '_completed.npy'))[0]\n",
    "\n",
    "comp_indices = []\n",
    "\n",
    "for i in tqdm(range(0, 99480, 10)):\n",
    "    if all(j in indices for j in range(i,i+10)):\n",
    "        comp_indices.extend(list(range(i,i+10)))\n",
    "\n",
    "masks = np.load(EVAL_DIR / 'mask.npy')[comp_indices[::10]]\n",
    "margins = np.load(EVAL_DIR / 'val_margins.npy')[comp_indices]\n",
    "margins = margins.reshape(len(margins) // 10,10,2000).mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ee710b-30e9-4bb1-9f4d-eb145b10b7d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba25042-2b4d-4501-b611-4ac7eb716727",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SS = scores_prev\n",
    "SS = scores.cpu().numpy()\n",
    "\n",
    "tmp_path = '.'\n",
    "# masks_url = 'https://www.dropbox.com/s/2nmcjaftdavyg0m/mask.npy?dl=1'\n",
    "# margins_url = 'https://www.dropbox.com/s/tc3r3c3kgna2h27/val_margins.npy?dl=1'\n",
    "\n",
    "# masks_path = Path(tmp_path).joinpath('mask.npy')\n",
    "# wget.download(masks_url, out=str(masks_path), bar=None)\n",
    "# # num masks, num train samples\n",
    "# masks = torch.as_tensor(np.load(masks_path, mmap_mode='r')).float()\n",
    "\n",
    "# margins_path = Path(tmp_path).joinpath('val_margins.npy')\n",
    "# wget.download(margins_url, out=str(margins_path), bar=None)\n",
    "# # num , num val samples\n",
    "# margins = torch.as_tensor(np.load(margins_path, mmap_mode='r'))\n",
    "\n",
    "val_inds = np.arange(2000)\n",
    "preds = masks @ SS\n",
    "rs = []\n",
    "ps = []\n",
    "for ind, j in tqdm(enumerate(val_inds)):\n",
    "    r, p = spearmanr(preds[:, ind], margins[:, j])\n",
    "    rs.append(r)\n",
    "    ps.append(p)\n",
    "rs, ps = np.array(rs), np.array(ps)\n",
    "print(f'Correlation: {rs.mean()} (avg p value {ps.mean()})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f4bf6e-cac3-4917-9900-249af7b3f737",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
